{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle as p\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from read_data import *\n",
    "from process_data import *\n",
    "from run import *\n",
    "from constants import *\n",
    "from hparams import hparams\n",
    "from utils import *\n",
    "import _locale\n",
    "_locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = p.load(open(\"./data/word_embeddings.p\", 'rb'))\n",
    "word_embeddings = np.array(word_embeddings)\n",
    "word2index = p.load(open(\"./data/vocab.p\", 'rb'))\n",
    "\n",
    "index2kwd, kwd2index, index2cnt = read_kwd_vocab(\"./data/train_kwd_vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = reverse_dict(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(hparams.HIDDEN_SIZE, word_embeddings, hparams.RNN_LAYERS,\n",
    "                         dropout=hparams.DROPOUT, update_wd_emb=hparams.UPDATE_WD_EMB)\n",
    "decoder = AttnDecoderRNN(hparams.HIDDEN_SIZE, len(word2index), word_embeddings, hparams.ATTN_TYPE,hparams.RNN_LAYERS, dropout=hparams.DROPOUT, update_wd_emb=hparams.UPDATE_WD_EMB,\n",
    "                             condition=hparams.DECODER_CONDITION_TYPE)\n",
    "kwd_predictor = get_predictor(word_embeddings, hparams)\n",
    "kwd_bridge = MLPBridge(hparams.HIDDEN_SIZE, hparams.MAX_KWD, hparams.HIDDEN_SIZE, len(word_embeddings[0]),\n",
    "                               norm_type=hparams.BRIDGE_NORM_TYPE, dropout=hparams.DROPOUT)\n",
    "if hparams.USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    kwd_predictor.cuda()\n",
    "    kwd_bridge.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using the pretrained ckpt\n",
    "# models = torch.load(\"./ckpt/s2s_D0.3_cnn_noneg_dropout_replace_fr.epoch59.models\")\n",
    "# hparams.load(\"./hparams/s2s_D0.3_cnn_noneg_dropout_replace_fr.json\")\n",
    "models = torch.load(\"./ckpt/##YOUR_MODEL##.models\")\n",
    "hparams.load(\"./hparams/##YOUR_MODEL##.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/kwd_filter_dict.json\", encoding=\"utf-8\") as f:\n",
    "    filter_dict = json.load(f)\n",
    "\n",
    "def make_filter_mask(post, filter_dict, kwd2index):\n",
    "    curr_kwd_filter_mask = [0 for i in range(len(kwd2index))]\n",
    "    for keys, to_filters in filter_dict.items():\n",
    "        if keys.startswith(\"@\") and keys.endswith(\"@\"):  # regex\n",
    "            if bool(re.search(keys[1:-1], post)):\n",
    "                for kwd0 in to_filters:\n",
    "                    curr_kwd_filter_mask[kwd2index[kwd0]] = -1e20\n",
    "        else:\n",
    "            for k in keys.split(\",\"):\n",
    "                if k in post:\n",
    "                    for kwd0 in to_filters:\n",
    "                        curr_kwd_filter_mask[kwd2index[kwd0]] = -1e20\n",
    "    return curr_kwd_filter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "encoder.load_state_dict(models[\"encoder\"])\n",
    "decoder.load_state_dict(models[\"decoder\"])\n",
    "kwd_predictor.load_state_dict(models[\"kwd_predictor\"])\n",
    "kwd_bridge.load_state_dict(models[\"kwd_bridge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = \"oster fpsthm2578 6-speed retractable cord hand mixer with clean start , black  .\"\n",
    "words0 = sent0.strip().lower().split()[:hparams.MAX_POST_LEN]\n",
    "# batch of size 1\n",
    "input_seqs = [[word2index[x] if x in word2index else word2index[UNK_token] for x in words0 ]]\n",
    "input_lens = [len(words0)]\n",
    "test_data = [[\"id0\"],input_seqs,input_lens,[None],[None],[0],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwd_filter_mask0 = make_filter_mask(sent0, filter_dict, kwd2index)\n",
    "kwd_filter_masks = [kwd_filter_mask0]  # the mask here is for filter out kwds\n",
    "test_data[-1] = kwd_filter_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[2658, 53710, 20790, 4961, 748, 133, 1042, 14, 139, 1025, 5, 90, 4]]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "input_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam import evaluate_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "BATCH:   0%|          | 0/1 [00:00<?, ?it/s]/home/zhiling/.local/lib/python3.6/site-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "BATCH: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]['what is the wattage of the beaters ? <EOS>\\n', 'what is the wattage of the mixer ? <EOS>\\n', 'what is the power of the beaters ? <EOS>\\n', 'what is the wattage for the beaters ? <EOS>\\n', 'what is the wattage of this beaters ? <EOS>\\n', 'does this mixer have a beater attachment ? <EOS>\\n']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hparams.BATCH_SIZE = 1\n",
    "out_seqs = evaluate_beam(word2index, index2word, encoder, decoder, kwd_predictor, kwd_bridge, test_data, hparams.MAX_QUES_LEN, \"./infer_out\", \"infer\", index2kwd, save_all_beam=True, infer=True)\n",
    "print(out_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "CLUSTER: 100%|██████████| 1/1 [00:00<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "hparams.KWD_CLUSTERS = 2\n",
    "kwd_edge_cnt = scipy.sparse.load_npz(\"./data/kwd_edges.npz\")\n",
    "kwd_clusters = get_cluster_kwds(kwd_predictor, test_data, kwd_edge_cnt, index2kwd, kwd2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "BATCH: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n['what is the wattage of the beaters ? <EOS>\\n', 'does this mixer have a beater attachment ? <EOS>\\n', 'what is the wattage of the mixer ? <EOS>\\n', 'does this model have a beater attachment ? <EOS>\\n', 'does this mixer have a whisk attachment ? <EOS>\\n', 'does this model have a <unk> attachment ? <EOS>\\n']\nBATCH: 100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n['how long is the cord ? <EOS>\\n', 'how long is the power cord ? <EOS>\\n', 'what is the power of the cord ? <EOS>\\n', 'what is the power of the beaters ? <EOS>\\n', 'how long is the cord <EOS>\\n', 'is the cord retractable ? <EOS>\\n']\n"
    }
   ],
   "source": [
    "hparams.DECODE_USE_KWD_LABEL = True\n",
    "out_seqs = []\n",
    "for i in range(hparams.KWD_CLUSTERS):\n",
    "    test_data[5] = kwd_clusters[i]\n",
    "    tmp_seqs = evaluate_beam(word2index, index2word, encoder, decoder, kwd_predictor, kwd_bridge, test_data, hparams.MAX_QUES_LEN, \"./infer_out\", \"infer\", index2kwd, save_all_beam=True, infer=True)\n",
    "    print(tmp_seqs)\n",
    "    out_seqs.extend(tmp_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    text = re.sub(r\"& (\\S+) ;\", r\"&\\1;\", text)\n",
    "    text = re.sub(r\"& # (\\S+) ;\", r\"&#\\1;\", text)\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"( <EOS>)\", \"\", clean_html(text.strip()))\n",
    "\n",
    "# Jaccard\n",
    "def sent_sim(text1, text2):\n",
    "    words1, words2 = set(text1.lower().strip().split()), set(text2.lower().strip().split())\n",
    "    return len(words1 & words2) / len(words1 | words2)\n",
    "\n",
    "def deduplicate(texts0, preserve=3, threshold=0.5):\n",
    "    texts = texts0[:]\n",
    "    assert len(texts) >= preserve and preserve > 0\n",
    "    if len(texts) == preserve:\n",
    "        return list(range(len(texts))), texts\n",
    "    sel_ids, remain_ids = [0], list(range(1, len(texts)))\n",
    "    sel_texts, remain_texts = texts[:1], texts[1:]\n",
    "    for i in range(1, preserve):\n",
    "        overlaps = []\n",
    "        sel_cand = None\n",
    "        for cand_id, cand in enumerate(remain_texts):\n",
    "            overlap = max(sent_sim(cand, sel) for sel in sel_texts)\n",
    "            if overlap < threshold:\n",
    "                sel_cand = cand_id\n",
    "                break\n",
    "            overlaps.append(overlap)\n",
    "        if sel_cand is None:\n",
    "            sel_cand = np.argmin(overlaps)\n",
    "        sel_texts.append(remain_texts[sel_cand])\n",
    "        sel_ids.append(remain_ids[sel_cand])\n",
    "        del remain_texts[sel_cand]\n",
    "        del remain_ids[sel_cand]\n",
    "    return sel_ids, sel_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['what is the wattage of the beaters ?', 'does this mixer have a beater attachment ?', 'how long is the cord ?']\n"
    }
   ],
   "source": [
    "cleaned_out_seqs = [clean_text(x) for x in out_seqs]\n",
    "filtered_ids, filtered_texts = deduplicate(cleaned_out_seqs)\n",
    "print(filtered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}